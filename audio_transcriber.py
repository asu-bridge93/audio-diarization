"""
éŸ³å£°æ–‡å­—èµ·ã“ã—å‡¦ç†ã®ã‚³ã‚¢ã‚¯ãƒ©ã‚¹
MP4å‹•ç”»ã‹ã‚‰éŸ³å£°æŠ½å‡º â†’ è©±è€…åˆ†é›¢ â†’ æ–‡å­—èµ·ã“ã— â†’ Markdownå‡ºåŠ›
"""

import torch
import warnings
import logging
import tempfile
import os
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from pyannote.audio import Pipeline, Audio
from moviepy.editor import VideoFileClip

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class AudioTranscriber:
    """MP4å‹•ç”»ã‹ã‚‰è©±è€…åˆ†é›¢ä»˜ãæ–‡å­—èµ·ã“ã—ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.device = self._get_device()
        self.diarization_pipeline = None
        self.whisper_processor = None
        self.whisper_model = None
        self.audio_handler = None
        
        logging.info(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
    
    def _get_device(self) -> torch.device:
        """æœ€é©ãªå‡¦ç†ãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•é¸æŠ"""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif torch.backends.mps.is_available():
            return torch.device("mps")
        return torch.device("cpu")
    
    def _load_models(self):
        """å¿…è¦ãªAIãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if self.diarization_pipeline is not None:
            return  # æ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿
        
        logging.info("AIãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # è©±è€…åˆ†é›¢ãƒ¢ãƒ‡ãƒ«
        self.diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1")
        self.diarization_pipeline.to(self.device)
        
        # Whisperæ–‡å­—èµ·ã“ã—ãƒ¢ãƒ‡ãƒ«
        self.whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3")
        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
            "openai/whisper-large-v3",
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
        ).to(self.device)
        self.whisper_model.eval()
        
        # éŸ³å£°å‡¦ç†ãƒãƒ³ãƒ‰ãƒ©
        self.audio_handler = Audio(sample_rate=16000, mono=True)
        
        logging.info("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    
    def extract_audio_from_mp4(self, mp4_path: Path) -> Path:
        """MP4å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºã—ã¦WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        logging.info(f"MP4ã‹ã‚‰éŸ³å£°æŠ½å‡ºä¸­: {mp4_path.name}")
        
        # ä¸€æ™‚éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        temp_fd, temp_path = tempfile.mkstemp(suffix='.wav', prefix='extracted_audio_')
        os.close(temp_fd)
        temp_audio_path = Path(temp_path)
        
        try:
            # MoviePyã§éŸ³å£°æŠ½å‡º
            with VideoFileClip(str(mp4_path)) as video:
                if video.audio is None:
                    raise ValueError("MP4ãƒ•ã‚¡ã‚¤ãƒ«ã«éŸ³å£°ãƒˆãƒ©ãƒƒã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
                
                video.audio.write_audiofile(
                    str(temp_audio_path),
                    codec='pcm_s16le',
                    verbose=False,
                    logger=None
                )
            
            logging.info("éŸ³å£°æŠ½å‡ºå®Œäº†")
            return temp_audio_path
            
        except Exception as e:
            if temp_audio_path.exists():
                temp_audio_path.unlink()
            raise RuntimeError(f"éŸ³å£°æŠ½å‡ºå¤±æ•—: {e}")
    
    def transcribe_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦è©±è€…åˆ†é›¢ã¨æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œ
        
        Args:
            file_path: MP4å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            è©±è€…åˆ†é›¢çµæœã®ãƒªã‚¹ãƒˆ [{'start': '00:00:05', 'end': '00:00:12', 'speaker': 'SPEAKER_00', 'text': '...'}]
        """
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self._load_models()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
        if file_path.suffix.lower() in ['.mp4', '.avi', '.mov', '.mkv', '.webm']:
            audio_path = self.extract_audio_from_mp4(file_path)
            temp_file = audio_path
        else:
            audio_path = file_path
            temp_file = None
        
        try:
            # è©±è€…åˆ†é›¢å®Ÿè¡Œ
            logging.info("è©±è€…åˆ†é›¢ã‚’å®Ÿè¡Œä¸­...")
            diarization_result = self.diarization_pipeline(str(audio_path))
            
            # å„è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ–‡å­—èµ·ã“ã—
            logging.info("æ–‡å­—èµ·ã“ã—ã‚’å®Ÿè¡Œä¸­...")
            transcription_results = []
            
            for segment, _, speaker_label in diarization_result.itertracks(yield_label=True):
                # çŸ­ã™ãã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if segment.duration < 0.5:
                    continue
                
                # éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
                waveform, sample_rate = self.audio_handler.crop(str(audio_path), segment)
                
                # æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
                transcribed_text = self._transcribe_audio_segment(waveform, sample_rate)
                
                if transcribed_text and transcribed_text.strip():
                    transcription_results.append({
                        'start': self._seconds_to_timestamp(segment.start),
                        'end': self._seconds_to_timestamp(segment.end),
                        'speaker': speaker_label,
                        'text': transcribed_text.strip()
                    })
            
            logging.info(f"å‡¦ç†å®Œäº†: {len(transcription_results)}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
            return transcription_results
            
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            if temp_file and temp_file.exists():
                temp_file.unlink()
    
    def _transcribe_audio_segment(self, waveform: torch.Tensor, sample_rate: int) -> Optional[str]:
        """éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’Whisperã§æ–‡å­—èµ·ã“ã—"""
        try:
            # Torch Tensorã‚’NumPyé…åˆ—ã«å¤‰æ›
            if waveform.ndim == 2:
                audio_array = waveform.squeeze(0).numpy()
            else:
                audio_array = waveform.numpy()
            
            # Whisperã§å‡¦ç†
            input_features = self.whisper_processor(
                audio_array,
                sampling_rate=sample_rate,
                return_tensors="pt"
            ).input_features.to(self.device)
            
            # æ—¥æœ¬èªæ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
            with torch.no_grad():
                generated_ids = self.whisper_model.generate(
                    input_features,
                    forced_decoder_ids=self.whisper_processor.get_decoder_prompt_ids(
                        language="ja", task="transcribe"
                    )
                )
            
            # ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
            transcribed_text = self.whisper_processor.batch_decode(
                generated_ids, skip_special_tokens=True
            )[0]
            
            return transcribed_text
            
        except Exception as e:
            logging.warning(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _seconds_to_timestamp(self, seconds: float) -> str:
        """ç§’æ•°ã‚’HH:MM:SSå½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›"""
        total_seconds = int(seconds)
        hours, remainder = divmod(total_seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{hours:02}:{minutes:02}:{seconds:02}"
    
    def create_markdown_report(self, results: List[Dict[str, Any]], input_filename: str) -> str:
        """
        æ–‡å­—èµ·ã“ã—çµæœã‚’Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ç”Ÿæˆ
        
        Args:
            results: transcribe_file()ã®æˆ»ã‚Šå€¤
            input_filename: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
            
        Returns:
            Markdownå½¢å¼ã®æ–‡å­—èµ·ã“ã—ãƒ¬ãƒãƒ¼ãƒˆ
        """
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        markdown_lines = [
            "# ğŸµ éŸ³å£°æ–‡å­—èµ·ã“ã—çµæœ",
            "",
            f"**ğŸ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«**: {input_filename}",
            f"**ğŸ“… å‡¦ç†æ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}",
            f"**ğŸ‘¥ è©±è€…æ•°**: {len(set(r['speaker'] for r in results))}å",
            f"**ğŸ“Š ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°**: {len(results)}å€‹",
            "",
            "---",
            "",
            "## ğŸ“ ç™ºè©±å†…å®¹",
            ""
        ]
        
        # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å†…å®¹
        for result in results:
            markdown_lines.extend([
                f"**[{result['start']} - {result['end']}] {result['speaker']}:**",
                f"> {result['text']}",
                ""
            ])
        
        return "\n".join(markdown_lines)
    
    def save_markdown_file(self, markdown_content: str, output_path: Path):
        """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        logging.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")